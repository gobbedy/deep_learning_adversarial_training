{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TensorFlow with GPU",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gobbedy/deep_learning_adversarial_training/blob/master/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BlmQIFSLZDdc"
      },
      "cell_type": "markdown",
      "source": [
        "# Confirm TensorFlow can see the GPU\n",
        "\n",
        "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3IEVK-KFxi5Z",
        "outputId": "21e65bf2-c656-4355-e663-178d0f35f120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7qiMcWxKQshg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQZo-r9iRthA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**MNIST**"
      ]
    },
    {
      "metadata": {
        "id": "pxqSj-Z6Qw44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "fd28fa47-a1a5-4dfc-9e7a-cf55b820fb98"
      },
      "cell_type": "code",
      "source": [
        "# Loading the data (signs)\n",
        "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "train_data = mnist.train.images  # Returns np.array\n",
        "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
        "eval_data = mnist.test.images  # Returns np.array\n",
        "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-1b6ff1cc1ee8>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g55HpX6WRkzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c4bd6832-5069-4830-e3a1-4e74e72237eb"
      },
      "cell_type": "code",
      "source": [
        "# Example of a picture\n",
        "train_data.shape\n",
        "index = 7\n",
        "plt.imshow(train_data[index].reshape(28, 28))\n",
        "print (\"y = \" + str(np.squeeze(train_labels[index])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSAAKukgACrpIAAq6SAAKukgACrpIAP8PFRI4ykIWcr8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f38b1d17dd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u07IiPMxRnZj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ca79da6d-d03b-444d-bdec-56672943ae8f"
      },
      "cell_type": "code",
      "source": [
        "print (\"number of training examples = \" + str(train_data.shape[0]))\n",
        "print (\"number of evaluation examples = \" + str(eval_data.shape[0]))\n",
        "print (\"X_train shape: \" + str(train_data.shape))\n",
        "print (\"Y_train shape: \" + str(train_labels.shape))\n",
        "print (\"X_test shape: \" + str(eval_data.shape))\n",
        "print (\"Y_test shape: \" + str(eval_labels.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 55000\n",
            "number of evaluation examples = 10000\n",
            "X_train shape: (55000, 784)\n",
            "Y_train shape: (55000,)\n",
            "X_test shape: (10000, 784)\n",
            "Y_test shape: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-m90eBTSK9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "VGGnet"
      ]
    },
    {
      "metadata": {
        "id": "ZABLTnT7VRWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_fn(features, labels, mode):\n",
        "    # Input Layer\n",
        "    input_height, input_width = 28, 28\n",
        "    input_channels = 1\n",
        "    input_layer = tf.reshape(features[\"x\"], [-1, input_height, input_width, input_channels])\n",
        "\n",
        "    # Convolutional Layer #1 and Pooling Layer #1\n",
        "    conv1_1 = tf.layers.conv2d(inputs=input_layer, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv1_2 = tf.layers.conv2d(inputs=conv1_1, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool1 = tf.layers.max_pooling2d(inputs=conv1_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "    \n",
        "    # Convolutional Layer #2 and Pooling Layer #2\n",
        "    conv2_1 = tf.layers.conv2d(inputs=pool1, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv2_2 = tf.layers.conv2d(inputs=conv2_1, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool2 = tf.layers.max_pooling2d(inputs=conv2_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # Convolutional Layer #3 and Pooling Layer #3\n",
        "    conv3_1 = tf.layers.conv2d(inputs=pool2, filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv3_2 = tf.layers.conv2d(inputs=conv3_1, filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool3 = tf.layers.max_pooling2d(inputs=conv3_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # Convolutional Layer #4 and Pooling Layer #4\n",
        "    conv4_1 = tf.layers.conv2d(inputs=pool3, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv4_2 = tf.layers.conv2d(inputs=conv4_1, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool4 = tf.layers.max_pooling2d(inputs=conv4_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # Convolutional Layer #5 and Pooling Layer #5\n",
        "    conv5_1 = tf.layers.conv2d(inputs=pool4, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv5_2 = tf.layers.conv2d(inputs=conv5_1, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool5 = tf.layers.max_pooling2d(inputs=conv5_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # FC Layers\n",
        "    pool5_flat = tf.contrib.layers.flatten(pool5)\n",
        "    FC1 = tf.layers.dense(inputs=pool5_flat, units=4096, activation=tf.nn.relu)\n",
        "    FC2 = tf.layers.dense(inputs=FC1, units=4096, activation=tf.nn.relu)\n",
        "    FC3 = tf.layers.dense(inputs=FC2, units=1000, activation=tf.nn.relu)\n",
        "\n",
        "    \"\"\"the training argument takes a boolean specifying whether or not the model is currently \n",
        "    being run in training mode; dropout will only be performed if training is true. here, \n",
        "    we check if the mode passed to our model function cnn_model_fn is train mode. \"\"\"\n",
        "    dropout = tf.layers.dropout(inputs=FC3, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    \n",
        "    # Logits Layer or the output layer. which will return the raw values for our predictions.\n",
        "    # Like FC layer, logits layer is another dense layer. We leave the activation function empty \n",
        "    # so we can apply the softmax\n",
        "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
        "    \n",
        "    # Then we make predictions based on raw output\n",
        "    predictions = {\n",
        "        # Generate predictions (for PREDICT and EVAL mode)\n",
        "        # the predicted class for each example - a vlaue from 0-9\n",
        "        \"classes\": tf.argmax(input=logits, axis=1),\n",
        "        # to calculate the probablities for each target class we use the softmax\n",
        "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "    }\n",
        "    \n",
        "    # so now our predictions are compiled in a dict object in python and using that we return an estimator object\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "    \n",
        "    \n",
        "    '''Calculate Loss (for both TRAIN and EVAL modes): computes the softmax entropy loss. \n",
        "    This function both computes the softmax activation function as well as the resulting loss.'''\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # Configure the Training Options (for TRAIN mode)\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
        "        \n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "    # Add evaluation metrics (for EVAL mode)\n",
        "    eval_metric_ops = {\n",
        "        \"accuracy\": tf.metrics.accuracy(labels=labels,\n",
        "                                        predictions=predictions[\"classes\"])}\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=loss,\n",
        "                                      eval_metric_ops=eval_metric_ops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIhZd72WR5o7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "57c7eb68-6b31-4136-bbf7-f366e752d7f8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the Estimator\n",
        "mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
        "                                          model_dir=\"/tmp/mnist_vgg13_model\")\n",
        "\n",
        "# Train the model\n",
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": train_data},\n",
        "                                                    y=train_labels,\n",
        "                                                    batch_size=100,\n",
        "                                                    num_epochs=100,\n",
        "                                                    shuffle=True)\n",
        "mnist_classifier.train(input_fn=train_input_fn,\n",
        "                       steps=None,\n",
        "                       hooks=None)\n",
        "\n",
        "# Evaluate the model and print results\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": eval_data},\n",
        "                                                   y=eval_labels,\n",
        "                                                   num_epochs=1,\n",
        "                                                   shuffle=False)\n",
        "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
        "print(eval_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f38738fb7d0>, '_model_dir': '/tmp/mnist_vgg13_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist_vgg13_model/model.ckpt-0\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/mnist_vgg13_model/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.3026614, step = 0\n",
            "INFO:tensorflow:global_step/sec: 12.1492\n",
            "INFO:tensorflow:loss = 2.3025632, step = 100 (8.237 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.302\n",
            "INFO:tensorflow:loss = 2.301976, step = 200 (8.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.3127\n",
            "INFO:tensorflow:loss = 2.3021634, step = 300 (8.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.3114\n",
            "INFO:tensorflow:loss = 2.3032289, step = 400 (8.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2955\n",
            "INFO:tensorflow:loss = 2.3021173, step = 500 (8.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2943\n",
            "INFO:tensorflow:loss = 2.3011272, step = 600 (8.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2991\n",
            "INFO:tensorflow:loss = 2.3014817, step = 700 (8.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2972\n",
            "INFO:tensorflow:loss = 2.3035336, step = 800 (8.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2886\n",
            "INFO:tensorflow:loss = 2.3023257, step = 900 (8.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2693\n",
            "INFO:tensorflow:loss = 2.302127, step = 1000 (8.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2316\n",
            "INFO:tensorflow:loss = 2.3001752, step = 1100 (8.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2779\n",
            "INFO:tensorflow:loss = 2.3021133, step = 1200 (8.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2887\n",
            "INFO:tensorflow:loss = 2.3015585, step = 1300 (8.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 12.2559\n",
            "INFO:tensorflow:loss = 2.2992492, step = 1400 (8.160 sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U_tpWHIQR-cw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}